\section{Materials \& Methods}
\subsection{Experimental Procedure and Setup \textcolor{green}{Sezen, Lukas, Marius, Klaus}}
Participants were informed of the nature of the experiment, recording and anonymization procedures and signed a consent form approved by the local ehtics committee of the Department of Psychology and Ergonomics at the TU Berlin. 20 participants (X female, mean age = X (Y sd)) were recruited through an online tool provided by the Department of Psychology and Ergonomics and through local listings. Participants were right-handed, had normal or corrected to normal vision and had no prior experience with vibrotactile feedback in virtual reality. Participants were compensated with 10 Euros per hour or study credits. Data of the first subject had to be removed from further analyses due to data recording error.

Using an HTC Vive VR Headset with the Vive Deluxe Audio Strap and a Vive Tracker (company details) attached to the right hand, a 3D object selection task was presented on a virtual table placed on an infinite white plane. The virtual environment was created in the Unity3D engine (Version, company details). White cubes appeared at random either in the center, to the left, or to the right of the participant, equidistant from a starting position. The time of a new cube spawning was randomized between 1-2 seconds after starting a trial. Participants were tasked to select the cube with their index finger and, upon completion, move their hand back to a resting position indicated on the table. The task was completed in two blocks of 300 trials each, with one block providing visual-only feedback, i.e. the cube changing its color from white to red upon selecting, and one block providing visual-tactile feedback, in which the selection contact was indicated by the color change plus a small vibrotactile pulse. Placed under the index fingertip, a vibration motor (Model \textit{308-100} from \textit{Precision Microdrives}), generating 0.8g at 200Hz and measuring 8mm in diameter was driven at 70mA by a 2N7000 MOSFET connected to an Arduino output pin at 3V. An initial 24 trial training session was followed by the two experimental blocks (balanced across participants), each followed by two questionnaires, NASA-TLX and IPQ. For the main experimental manipulation of asynchrony, 25\% of the trials (equaling 75 asynchronous trials per feedback block) exhibited spatio-temporal asynchrony in line with established oddball paradigms. Object selection was triggered prematurely by bounding a spherical collider to the cube and enlarging it by 350\% in comparison to a collider bounded to the shape of the cube in the synchronous trials. Asynchronous trials were sorted in a pseudo-randomized sequence following synchronous trials, i.e. between one and five synchronous trials preceeded an asynchronous trial. Extended task and apparatus descriptions can be found elsewhere \cite{Gehrke_2019}.
% todo add movie, figure and references

\subsection{Motion Capture and EEG Recording \textcolor{green}{Sezen, Lukas, Marius, Klaus}}
EEG was recorded using 64 active Ag/AgCl electrodes placed according to the extended international 10â€“20 system \cite{chatrian_ten_1985}. The electrode at position FP2 was detached from the cap and placed under the left eye (EOG). Impedances were kept under 30 \si{\kohm}. EEG was sampled at 500 Hz and amplified using BrainAmp DC amplifiers (Brainproducts GmbH, Gilching, Germany). Hand and head movements were sampled at 90 Hz when coming out of the HTC Vive processing cascade. Samples were recorded and synchronized using labstreaminglayer \footnote{https://github.com/sccn/labstreaminglayer}.                                                                                                                                                                                                                                                                                                                                                                                                                                                     Motion capture data was filtered with a 6Hz lowpass filter and upsampled to match EEG frequency using MoBILAB routines for concurrent analyses \cite{Ojeda2014}.